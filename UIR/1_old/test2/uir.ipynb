{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "from conllu.models import TokenList, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children_tokens(token_head: Token, group_token: TokenList):\n",
    "    children_tokens = []\n",
    "    for token in group_token:\n",
    "        if token['head'] == token_head['id'] and token['deprel'] != '_':\n",
    "            children_tokens.append(token)\n",
    "            children_tokens.extend(get_children_tokens(token, group_token))\n",
    "\n",
    "    return children_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_one_step_children_token(token_head: Token, group_token: TokenList):\n",
    "    one_step_children_tokens = []\n",
    "    for token in group_token:\n",
    "        if token['head'] == token_head['id'] and token['deprel'] != '_':\n",
    "            one_step_children_tokens.append(token)\n",
    "\n",
    "    return one_step_children_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_tokens(token_head: Token, group_token: TokenList):\n",
    "    phrase_tokens = []\n",
    "    for token in group_token:\n",
    "        if token['head'] == token_head['id'] and token['deprel'] == '_':\n",
    "            if token['form'] != '_':\n",
    "                phrase_tokens.append(token)\n",
    "            else:\n",
    "                phrase_tokens.extend(get_phrase_tokens(token, group_token))\n",
    "            phrase_tokens.extend(get_children_tokens(token, group_token))\n",
    "\n",
    "    return phrase_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(group_token: TokenList) -> list:\n",
    "    pairs = []\n",
    "    for token in group_token:\n",
    "        if token['head'] != 0 and token['head'] != 1:\n",
    "            if token['id'] < token['head']:\n",
    "                pairs.append([token['id'], token['head']])\n",
    "            else:\n",
    "                pairs.append([token['head'], token['id']])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ssg(id: int, upos: str, xpos: str, head: int, deprel: str) -> Token:\n",
    "    ssg_token = Token(\n",
    "        id = id,\n",
    "        form = '_',\n",
    "        lemma = '_',\n",
    "        upos = upos,\n",
    "        xpos = xpos,\n",
    "        feats = None,\n",
    "        head = head,\n",
    "        deprel = deprel,\n",
    "        deps = None,\n",
    "        misc = None\n",
    "        )\n",
    "\n",
    "    return ssg_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_root_token(group_token: TokenList) -> TokenList:\n",
    "    sentence = \"\"\n",
    "    for token in group_token:\n",
    "        sentence += token['form'] + ' '\n",
    "        if token['head'] == 0:\n",
    "            token['head'] = len(group_token) + 1\n",
    "    sentence = sentence[:-1]\n",
    "\n",
    "    root_token = create_ssg(len(group_token) + 1, sentence, '*', 0, '_')\n",
    "    group_token.append(root_token)\n",
    "\n",
    "    return group_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_root_token(group_token: TokenList) -> TokenList:\n",
    "    sentence = \"\"\n",
    "    for token in group_token:\n",
    "        sentence += token['form'] + ' '\n",
    "        token['id'] += 1\n",
    "        token['head'] += 1\n",
    "    sentence = sentence[:-1]\n",
    "\n",
    "    root_token = create_ssg(1, sentence, '*', 0, '_')\n",
    "    group_token.append(root_token)\n",
    "\n",
    "    group_token.sort(key=lambda token: token['id'])\n",
    "\n",
    "    return group_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feat_token(token: Token, feat: str) -> bool:\n",
    "    if token['feats'] != None:\n",
    "        if feat in str(list(token['feats'].keys())[0]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"input_test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "group_token = sentences[0]\n",
    "\n",
    "group_token = add_root_token(group_token)\n",
    "\n",
    "sentences[0] = group_token\n",
    "\n",
    "with open('output_test.conllu', mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([sentence.serialize() + \"\\n\" for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"input_test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "#for i in range(len(sentences)):\n",
    "sentence = sentences[0]\n",
    "sentence = add_root_token(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_list_one_step_children_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mif\u001b[39;00m token[\u001b[39m'\u001b[39m\u001b[39mupos\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m check_feat_token(token, \u001b[39m'\u001b[39m\u001b[39mИНФ\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     verb_token \u001b[39m=\u001b[39m token\n\u001b[1;32m----> 5\u001b[0m     list_one_step_children_token \u001b[39m=\u001b[39m get_list_one_step_children_token(token, sentence)\n\u001b[0;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m child_token \u001b[39min\u001b[39;00m list_one_step_children_token:\n\u001b[0;32m      7\u001b[0m         \u001b[39mif\u001b[39;00m child_token[\u001b[39m'\u001b[39m\u001b[39mupos\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_list_one_step_children_token' is not defined"
     ]
    }
   ],
   "source": [
    "bool_new_group = False\n",
    "for token in sentence:\n",
    "    if token['upos'] == 'VERB' and not check_feat_token(token, 'ИНФ'):\n",
    "        verb_token = token\n",
    "        list_one_step_children_token = get_list_one_step_children_token(token, sentence)\n",
    "        for child_token in list_one_step_children_token:\n",
    "            if child_token['upos'] == 'NOUN':\n",
    "                group = [verb_token, child_token]\n",
    "\n",
    "                pairs = get_pairs(sentence)\n",
    "                noun_pairs = []\n",
    "                for pair in pairs:\n",
    "                    if child_token['id'] in pair and child_token['head'] not in pair:\n",
    "                        noun_pairs.append(pair)\n",
    "                        pairs.remove(pair)\n",
    "                        print(pair)\n",
    "\n",
    "                qnt_noun_pairs = len(noun_pairs)\n",
    "                for noun_pair in noun_pairs:\n",
    "                    if child_token['head'] > noun_pair[0] and child_token['head'] < noun_pair[1]:\n",
    "                        print('noun_pair:', noun_pair)\n",
    "                        print('====================')\n",
    "                        noun_pairs.remove(noun_pair)\n",
    "                        continue\n",
    "                    for pair in pairs:\n",
    "                        if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                            print('pair:', pair)\n",
    "                            print('inf_pair:', inf_pair)\n",
    "                            noun_pairs.remove(noun_pair)\n",
    "\n",
    "                if qnt_noun_pairs > len(noun_pairs):\n",
    "                    bool_new_group = True\n",
    "\n",
    "                    group_token = []\n",
    "                    group_token.append(sentence[child_token['head'] - 1])\n",
    "                    group_token.append(child_token)\n",
    "                    for noun_pair in noun_pairs:\n",
    "                        if noun_pair[0] != child_token['id']:\n",
    "                            group_token.append(sentence[noun_pair[0] - 1])\n",
    "                        else:\n",
    "                            group_token.append(sentence[noun_pair[1] - 1])\n",
    "                    #print(group_token)\n",
    "\n",
    "                    #получить текст группы\n",
    "                    group_token.sort(key=lambda token: token['id'])\n",
    "                    phrase = ''\n",
    "                    for token in group_token:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "\n",
    "                    list_one_step_children_token = []\n",
    "                    for token in group_token:\n",
    "                        list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                    for token in list_one_step_children_token:\n",
    "                        if token not in group_token:\n",
    "                            #print(token)\n",
    "                            token['head'] = len(sentence) + 1\n",
    "                    sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_6', verb_token['head'], '_'))\n",
    "                    verb_token['head'] = len(sentence)\n",
    "\n",
    "    ##if not bool_new_group:\n",
    "    #    pass\n",
    "\n",
    "    if check_feat_token(token, 'ИНФ') and not check_feat_token(sentence[token['head'] - 1], 'ИНФ'):\n",
    "        inf_token = token\n",
    "        #print(inf_token)\n",
    "\n",
    "        group_token = []\n",
    "        head_token = inf_token\n",
    "        inf_group_token = []\n",
    "        print('222')\n",
    "        while head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "            print(head_token)\n",
    "            print('111')\n",
    "            root_token = head_token\n",
    "            if check_feat_token(head_token, 'ИНФ'):\n",
    "                #print(head_token)\n",
    "                inf_group_token.append(head_token)\n",
    "            #print(head_token)\n",
    "            group_token.append(head_token)\n",
    "            head_token = sentence[head_token['head'] - 1]\n",
    "            print(head_token['head'])\n",
    "            #print(head_token)\n",
    "\n",
    "        child_token = inf_token\n",
    "        bool_child = True\n",
    "        while bool_child:\n",
    "            bool_child = False\n",
    "            for token in sentence:\n",
    "                if token['head'] == child_token['id']:\n",
    "                    if check_feat_token(token, 'ИНФ'):\n",
    "                        #print(token)\n",
    "                        inf_group_token.append(token)\n",
    "                        group_token.append(token)\n",
    "                        child_token = token\n",
    "                        bool_child = True\n",
    "                        break\n",
    "        list_one_step_children_token = get_list_one_step_children_token(child_token, sentence)\n",
    "        bool_noun = False\n",
    "        for child_token in list_one_step_children_token:\n",
    "            if child_token['upos'] == 'NOUN':\n",
    "                noun_token = child_token\n",
    "                bool_noun = True\n",
    "                group_token.append(noun_token)\n",
    "                break\n",
    "\n",
    "        if bool_noun:\n",
    "            pairs = get_pairs(sentence)\n",
    "            noun_pairs = []\n",
    "            for pair in pairs:\n",
    "                if child_token['id'] in pair and child_token['head'] not in pair:\n",
    "                    noun_pairs.append(pair)\n",
    "                    pairs.remove(pair)\n",
    "                    #print(pair)\n",
    "\n",
    "            qnt_noun_pairs = len(noun_pairs)\n",
    "            for noun_pair in noun_pairs:\n",
    "                if child_token['head'] > noun_pair[0] and child_token['head'] < noun_pair[1]:\n",
    "                    #print('noun_pair:', noun_pair)\n",
    "                    #print('====================')\n",
    "                    noun_pairs.remove(noun_pair)\n",
    "                    continue\n",
    "                for pair in pairs:\n",
    "                    if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                        #print('pair:', pair)\n",
    "                        #print('inf_pair:', inf_pair)\n",
    "                        noun_pairs.remove(noun_pair)\n",
    "\n",
    "            if qnt_noun_pairs > len(noun_pairs):\n",
    "                bool_new_group = True\n",
    "\n",
    "                group_token.sort(key=lambda token: token['id'])\n",
    "                phrase = ''\n",
    "                for token in group_token:\n",
    "                    phrase += token['form'] + ' '\n",
    "                phrase = phrase[:-1]\n",
    "                #print(phrase)\n",
    "\n",
    "                list_one_step_children_token = []\n",
    "                for token in group_token:\n",
    "                    list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                #list_one_step_children_token = []\n",
    "                #for token in group_token:\n",
    "                #    for buf_token in get_list_one_step_children_token(token, sentence):\n",
    "                #        if buf_token not in group_token:\n",
    "                #            list_one_step_children_token.extend(buf_token)\n",
    "\n",
    "                for token in list_one_step_children_token:\n",
    "                    if token not in group_token:\n",
    "                        #print(token)\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_6', root_token['head'], '_'))\n",
    "                root_token['head'] = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_test.conllu', mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([sentence.serialize() + \"\\n\" for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_5(sentence: TokenList):\n",
    "    for token in sentence:\n",
    "        bool_new_group = False\n",
    "\n",
    "        if check_feat_token(token, 'ИНФ') and not check_feat_token(sentence[token['head'] - 1], 'ИНФ'):\n",
    "            inf_token = token\n",
    "            #print(inf_token)\n",
    "\n",
    "            group_token = []\n",
    "            head_token = inf_token\n",
    "            inf_group_token = []\n",
    "            while head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "                root_token = head_token\n",
    "                if check_feat_token(head_token, 'ИНФ'):\n",
    "                    #print(head_token)\n",
    "                    inf_group_token.append(head_token)\n",
    "                #print(head_token)\n",
    "                group_token.append(head_token)\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "            child_token = inf_token\n",
    "            bool_child = True\n",
    "            while bool_child:\n",
    "                bool_child = False\n",
    "                for token in sentence:\n",
    "                    if token['head'] == child_token['id']:\n",
    "                        if check_feat_token(token, 'ИНФ'):\n",
    "                            print(token)\n",
    "                            inf_group_token.append(token)\n",
    "                            group_token.append(token)\n",
    "                            child_token = token\n",
    "                            bool_child = True\n",
    "                            break\n",
    "\n",
    "            pairs = get_pairs(sentence)\n",
    "            #print(pairs)\n",
    "            inf_pairs = []\n",
    "            for token in inf_group_token:\n",
    "                for pair in pairs:\n",
    "                    #print(pair)\n",
    "                    if token['id'] in pair and token['head'] not in pair:\n",
    "                        inf_pairs.append(pair)\n",
    "                        pairs.remove(pair)\n",
    "            #print(inf_pairs)\n",
    "            #print(pairs)\n",
    "\n",
    "            print('pairs:', pairs)\n",
    "            print('inf_pairs:', inf_pairs)\n",
    "            qnt_inf_pairs = len(inf_pairs)\n",
    "            for inf_pair in inf_pairs:\n",
    "                if inf_token['head'] > inf_pair[0] and inf_token['head'] < inf_pair[1]:\n",
    "                    print('inf_pair:', inf_pair)\n",
    "                    print('====================')\n",
    "                    inf_pairs.remove(inf_pair)\n",
    "                    continue\n",
    "                for pair in pairs:\n",
    "                    if (inf_pair[0] > pair[0] and inf_pair[0] < pair[1] and (inf_pair[1] < pair[0] or inf_pair[1] > pair[1])) or (inf_pair[1] > pair[0] and inf_pair[1] < pair[1] and (inf_pair[0] < pair[0] or inf_pair[0] > pair[1])):\n",
    "                        print('pair:', pair)\n",
    "                        print('inf_pair:', inf_pair)\n",
    "                        inf_pairs.remove(inf_pair)\n",
    "\n",
    "            print('inf_pairs:', inf_pairs)\n",
    "            if qnt_inf_pairs > len(inf_pairs):\n",
    "                bool_new_group = True\n",
    "\n",
    "                print(len(group_token))\n",
    "                for inf_pair in inf_pairs:\n",
    "                    print(inf_pair)\n",
    "                    bool_inf_0 = True\n",
    "                    bool_inf_1 = True\n",
    "                    for inf_token in inf_group_token:\n",
    "                        if inf_pair[0] == inf_token['id']:\n",
    "                            bool_inf_0 = False\n",
    "                        if inf_pair[1] == inf_token['id']:\n",
    "                            bool_inf_1 = False\n",
    "                    if bool_inf_0:\n",
    "                        group_token.append(sentence[inf_pair[0] - 1])\n",
    "                    if bool_inf_1:\n",
    "                        group_token.append(sentence[inf_pair[1] - 1])\n",
    "                print(len(group_token))\n",
    "\n",
    "                group_token.sort(key=lambda token: token['id'])\n",
    "                phrase = ''\n",
    "                for token in group_token:\n",
    "                    phrase += token['form'] + ' '\n",
    "                phrase = phrase[:-1]\n",
    "                print(phrase)\n",
    "\n",
    "                list_one_step_children_token = []\n",
    "                for token in group_token:\n",
    "                    list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                #list_one_step_children_token = []\n",
    "                #for token in group_token:\n",
    "                #    for buf_token in get_list_one_step_children_token(token, sentence):\n",
    "                #        if buf_token not in group_token:\n",
    "                #            list_one_step_children_token.extend(buf_token)\n",
    "\n",
    "                for token in list_one_step_children_token:\n",
    "                    if token not in group_token:\n",
    "                        print(token)\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_5', root_token['head'], '_'))\n",
    "                root_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_3(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "        #Поиск ГЛ, который не является ИНФ\n",
    "        if token['upos'] == 'VERB' and not check_feat_token(token, 'ИНФ'):\n",
    "            verb_token = token\n",
    "\n",
    "            #Поиск ИНФ, который зависит от найденного ГЛ\n",
    "            list_one_step_children_token = get_list_one_step_children_token(verb_token, sentence)\n",
    "            inf_token = None\n",
    "            for child_token in list_one_step_children_token:\n",
    "                if check_feat_token(child_token, 'ИНФ'):\n",
    "                    inf_token = child_token\n",
    "                    break\n",
    "            if inf_token == None:\n",
    "                continue\n",
    "\n",
    "            #Поиск двух наборов пар связей (в первой все связи, в которых отсуствует найденный ИНФ, а во второй - присуствует)\n",
    "            pairs = get_pairs(sentence)\n",
    "            inf_pairs = []\n",
    "            for pair in pairs:\n",
    "                if inf_token['id'] in pair and inf_token['head'] not in pair:\n",
    "                    inf_pairs.append(pair)\n",
    "                    pairs.remove(pair)\n",
    "\n",
    "            #Удалить все пары с ИНФ, которые нарушают проективность\n",
    "            qnt_inf_pairs = len(inf_pairs)\n",
    "            for inf_pair in inf_pairs:\n",
    "                if inf_token['head'] > inf_pair[0] and inf_token['head'] < inf_pair[1]:\n",
    "                    inf_pairs.remove(inf_pair)\n",
    "                    continue\n",
    "                for pair in pairs:\n",
    "                    if (inf_pair[0] > pair[0] and inf_pair[0] < pair[1] and (inf_pair[1] < pair[0] or inf_pair[1] > pair[1])) or (inf_pair[1] > pair[0] and inf_pair[1] < pair[1] and (inf_pair[0] < pair[0] or inf_pair[0] > pair[1])):\n",
    "                        inf_pairs.remove(inf_pair)\n",
    "\n",
    "            #Проверка, что нужно добавлять новую группу\n",
    "            if qnt_inf_pairs > len(inf_pairs):\n",
    "                bool_new_group = True\n",
    "\n",
    "                #Получить набор слов, которые образуют новую группу\n",
    "                group_token = [verb_token, inf_token]\n",
    "                for inf_pair in inf_pairs:\n",
    "                    if inf_pair[0] != inf_token['id']:\n",
    "                        group_token.append(sentence[inf_pair[0] - 1])\n",
    "                    else:\n",
    "                        group_token.append(sentence[inf_pair[1] - 1])\n",
    "\n",
    "                #Получить текст группы\n",
    "                group_token.sort(key=lambda token: token['id'])\n",
    "                phrase = ''\n",
    "                for token in group_token:\n",
    "                    phrase += token['form'] + ' '\n",
    "                phrase = phrase[:-1]\n",
    "\n",
    "                #Получить все слова, которые связаны со словами из группы\n",
    "                list_one_step_children_token = []\n",
    "                for token in group_token:\n",
    "                    list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                #Добавить новую группу и изменить связи между элементами ССГ\n",
    "                for token in list_one_step_children_token:\n",
    "                    if token not in group_token:\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г3', verb_token['head'], '_'))\n",
    "                verb_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_4(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "        #поиск ГЛ, который не является ИНФ\n",
    "        if token['upos'] in ['NOUN', 'ADJ', 'ADV']:\n",
    "            id_head_token = token['id']\n",
    "            #print('1', token)\n",
    "            root_token = token\n",
    "\n",
    "            #if sentence[token['head'] - 1]['form'] == '_':\n",
    "            #    root_token = sentence[token['head'] - 1]\n",
    "            #    print('2', root_token['upos'])\n",
    "\n",
    "            #поиск ИНФ, зависимого от найденного слова\n",
    "            list_one_step_children_token = get_list_one_step_children_token(root_token, sentence)\n",
    "            inf_token = None\n",
    "            for child_token in list_one_step_children_token:\n",
    "                if check_feat_token(child_token, 'ИНФ'):\n",
    "                    inf_token = child_token\n",
    "                    #print(inf_token)\n",
    "                    break\n",
    "            if inf_token == None:\n",
    "                continue\n",
    "\n",
    "            #поиск двух наборов пар связей (в первой все связи, в которых отсуствует найденный ИНФ, а во второй - присуствует)\n",
    "            pairs = get_pairs(sentence)\n",
    "            inf_pairs = []\n",
    "            for pair in pairs:\n",
    "                if inf_token['id'] in pair and inf_token['head'] not in pair:\n",
    "                    inf_pairs.append(pair)\n",
    "                    pairs.remove(pair)\n",
    "            #print(pairs)\n",
    "            #print(inf_pairs)\n",
    "            #print(inf_token)\n",
    "\n",
    "            #удалить все пары с ИНФ, которые нарушают проективность\n",
    "            qnt_inf_pairs = len(inf_pairs)\n",
    "            for inf_pair in inf_pairs:\n",
    "                if id_head_token > inf_pair[0] and id_head_token < inf_pair[1]:\n",
    "                    #print('inf_pair:', inf_pair)\n",
    "                    #print('====================')\n",
    "                    inf_pairs.remove(inf_pair)\n",
    "                    continue\n",
    "                for pair in pairs:\n",
    "                    if (inf_pair[0] > pair[0] and inf_pair[0] < pair[1] and (inf_pair[1] < pair[0] or inf_pair[1] > pair[1])) or (inf_pair[1] > pair[0] and inf_pair[1] < pair[1] and (inf_pair[0] < pair[0] or inf_pair[0] > pair[1])):\n",
    "                        #print('pair:', pair)\n",
    "                        #print('inf_pair:', inf_pair)\n",
    "                        inf_pairs.remove(inf_pair)\n",
    "\n",
    "            #проверка, что нужно добавлять новую группу\n",
    "            if qnt_inf_pairs > len(inf_pairs):\n",
    "                bool_new_group = True\n",
    "\n",
    "                #получить группу слов, которые образуют новую группу\n",
    "                group_token = []\n",
    "                group_token.append(sentence[inf_token['head'] - 1])\n",
    "                group_token.append(inf_token)\n",
    "                for inf_pair in inf_pairs:\n",
    "                    if inf_pair[0] != inf_token['id']:\n",
    "                        group_token.append(sentence[inf_pair[0] - 1])\n",
    "                    else:\n",
    "                        group_token.append(sentence[inf_pair[1] - 1])\n",
    "                #print('rrr')\n",
    "                #print(group_token)\n",
    "\n",
    "                #получить текст группы\n",
    "                #for token in group_token:\n",
    "                #    print('t', token)\n",
    "                #    if token['form'] == '_':\n",
    "                #        group_token.append(get_list_token_from_phrase(token, sentence))\n",
    "                group_token.sort(key=lambda token: token['id'])\n",
    "                phrase = ''\n",
    "                for token in group_token:\n",
    "                    phrase += token['form'] + ' '\n",
    "                phrase = phrase[:-1]\n",
    "\n",
    "                #получить все слова, которые связаны со словами из группы\n",
    "                list_one_step_children_token = []\n",
    "                for token in group_token:\n",
    "                    list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                #добавить новую группу и изменить связи между элементами ССГ\n",
    "                for token in list_one_step_children_token:\n",
    "                    if token not in group_token:\n",
    "                        #print(token)\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_4', root_token['head'], '_'))\n",
    "                root_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_5(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "\n",
    "        if check_feat_token(token, 'ИНФ') and not check_feat_token(sentence[token['head'] - 1], 'ИНФ'):\n",
    "            inf_token = token\n",
    "            #print(inf_token)\n",
    "\n",
    "            group_token = []\n",
    "            head_token = inf_token\n",
    "            inf_group_token = []\n",
    "            while head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "                root_token = head_token\n",
    "                if check_feat_token(head_token, 'ИНФ'):\n",
    "                    #print(head_token)\n",
    "                    inf_group_token.append(head_token)\n",
    "                #print(head_token)\n",
    "                group_token.append(head_token)\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "                #print(head_token)\n",
    "\n",
    "\n",
    "            child_token = inf_token\n",
    "            bool_child = True\n",
    "            while bool_child:\n",
    "                bool_child = False\n",
    "                for token in sentence:\n",
    "                    if token['head'] == child_token['id']:\n",
    "                        if check_feat_token(token, 'ИНФ'):\n",
    "                            #print(token)\n",
    "                            inf_group_token.append(token)\n",
    "                            group_token.append(token)\n",
    "                            child_token = token\n",
    "                            bool_child = True\n",
    "                            break\n",
    "\n",
    "            if len(group_token) >= 3:\n",
    "                pairs = get_pairs(sentence)\n",
    "                #print(pairs)\n",
    "                inf_pairs = []\n",
    "                for token in inf_group_token:\n",
    "                    for pair in pairs:\n",
    "                        #print(pair)\n",
    "                        if token['id'] in pair and token['head'] not in pair:\n",
    "                            inf_pairs.append(pair)\n",
    "                            pairs.remove(pair)\n",
    "                #print(inf_pairs)\n",
    "                #print(pairs)\n",
    "\n",
    "                #print('pairs:', pairs)\n",
    "                #print('inf_pairs:', inf_pairs)\n",
    "                qnt_inf_pairs = len(inf_pairs)\n",
    "                for inf_pair in inf_pairs:\n",
    "                    if inf_token['head'] > inf_pair[0] and inf_token['head'] < inf_pair[1]:\n",
    "                        #print('inf_pair:', inf_pair)\n",
    "                        #print('====================')\n",
    "                        inf_pairs.remove(inf_pair)\n",
    "                        continue\n",
    "                    for pair in pairs:\n",
    "                        if (inf_pair[0] > pair[0] and inf_pair[0] < pair[1] and (inf_pair[1] < pair[0] or inf_pair[1] > pair[1])) or (inf_pair[1] > pair[0] and inf_pair[1] < pair[1] and (inf_pair[0] < pair[0] or inf_pair[0] > pair[1])):\n",
    "                            #print('pair:', pair)\n",
    "                            #print('inf_pair:', inf_pair)\n",
    "                            inf_pairs.remove(inf_pair)\n",
    "\n",
    "                #print('inf_pairs:', inf_pairs)\n",
    "                if qnt_inf_pairs > len(inf_pairs):\n",
    "                    bool_new_group = True\n",
    "\n",
    "                    #print(len(group_token))\n",
    "                    for inf_pair in inf_pairs:\n",
    "                        #print(inf_pair)\n",
    "                        bool_inf_0 = True\n",
    "                        bool_inf_1 = True\n",
    "                        for inf_token in inf_group_token:\n",
    "                            if inf_pair[0] == inf_token['id']:\n",
    "                                bool_inf_0 = False\n",
    "                            if inf_pair[1] == inf_token['id']:\n",
    "                                bool_inf_1 = False\n",
    "                        if bool_inf_0:\n",
    "                            group_token.append(sentence[inf_pair[0] - 1])\n",
    "                        if bool_inf_1:\n",
    "                            group_token.append(sentence[inf_pair[1] - 1])\n",
    "                    #print(len(group_token))\n",
    "\n",
    "                    group_token.sort(key=lambda token: token['id'])\n",
    "                    phrase = ''\n",
    "                    for token in group_token:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "                    #print(phrase)\n",
    "\n",
    "                    list_one_step_children_token = []\n",
    "                    for token in group_token:\n",
    "                        list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                    #list_one_step_children_token = []\n",
    "                    #for token in group_token:\n",
    "                    #    for buf_token in get_list_one_step_children_token(token, sentence):\n",
    "                    #        if buf_token not in group_token:\n",
    "                    #            list_one_step_children_token.extend(buf_token)\n",
    "\n",
    "                    for token in list_one_step_children_token:\n",
    "                        if token not in group_token:\n",
    "                            #print(token)\n",
    "                            token['head'] = len(sentence) + 1\n",
    "                    sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_5', root_token['head'], '_'))\n",
    "                    root_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_6(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "        if token['upos'] == 'VERB' and not check_feat_token(token, 'ИНФ'):\n",
    "            verb_token = token\n",
    "            list_one_step_children_token = get_list_one_step_children_token(token, sentence)\n",
    "            for child_token in list_one_step_children_token:\n",
    "                if child_token['upos'] == 'NOUN':\n",
    "                    group = [verb_token, child_token]\n",
    "\n",
    "                    pairs = get_pairs(sentence)\n",
    "                    noun_pairs = []\n",
    "                    for pair in pairs:\n",
    "                        if child_token['id'] in pair and child_token['head'] not in pair:\n",
    "                            noun_pairs.append(pair)\n",
    "                            pairs.remove(pair)\n",
    "\n",
    "                    qnt_noun_pairs = len(noun_pairs)\n",
    "                    for noun_pair in noun_pairs:\n",
    "                        if child_token['head'] > noun_pair[0] and child_token['head'] < noun_pair[1]:\n",
    "                            noun_pairs.remove(noun_pair)\n",
    "                            continue\n",
    "                        for pair in pairs:\n",
    "                            if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                                noun_pairs.remove(noun_pair)\n",
    "\n",
    "                    if qnt_noun_pairs > len(noun_pairs):\n",
    "                        bool_new_group = True\n",
    "\n",
    "                        group_token = []\n",
    "                        group_token.append(sentence[child_token['head'] - 1])\n",
    "                        group_token.append(child_token)\n",
    "                        for noun_pair in noun_pairs:\n",
    "                            if noun_pair[0] != child_token['id']:\n",
    "                                group_token.append(sentence[noun_pair[0] - 1])\n",
    "                            else:\n",
    "                                group_token.append(sentence[noun_pair[1] - 1])\n",
    "\n",
    "                        group_token.sort(key=lambda token: token['id'])\n",
    "                        phrase = ''\n",
    "                        for token in group_token:\n",
    "                            phrase += token['form'] + ' '\n",
    "                        phrase = phrase[:-1]\n",
    "\n",
    "                        list_one_step_children_token = []\n",
    "                        for token in group_token:\n",
    "                            list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                        for token in list_one_step_children_token:\n",
    "                            if token not in group_token:\n",
    "                                #print(token)\n",
    "                                token['head'] = len(sentence) + 1\n",
    "                        sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_6', verb_token['head'], '_'))\n",
    "                        verb_token['head'] = len(sentence)\n",
    "\n",
    "\n",
    "        if check_feat_token(token, 'ИНФ') and not check_feat_token(sentence[token['head'] - 1], 'ИНФ'):\n",
    "            inf_token = token\n",
    "\n",
    "            group_token = []\n",
    "            head_token = inf_token\n",
    "            inf_group_token = []\n",
    "            print('222')\n",
    "            while head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "                print(head_token)\n",
    "                print('111')\n",
    "                root_token = head_token\n",
    "                if check_feat_token(head_token, 'ИНФ'):\n",
    "                    inf_group_token.append(head_token)\n",
    "                group_token.append(head_token)\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "                print(head_token['head'])\n",
    "\n",
    "            child_token = inf_token\n",
    "            bool_child = True\n",
    "            while bool_child:\n",
    "                bool_child = False\n",
    "                for token in sentence:\n",
    "                    if token['head'] == child_token['id']:\n",
    "                        if check_feat_token(token, 'ИНФ'):\n",
    "                            #print(token)\n",
    "                            inf_group_token.append(token)\n",
    "                            group_token.append(token)\n",
    "                            child_token = token\n",
    "                            bool_child = True\n",
    "                            break\n",
    "            list_one_step_children_token = get_list_one_step_children_token(child_token, sentence)\n",
    "            bool_noun = False\n",
    "            for child_token in list_one_step_children_token:\n",
    "                if child_token['upos'] == 'NOUN':\n",
    "                    noun_token = child_token\n",
    "                    bool_noun = True\n",
    "                    group_token.append(noun_token)\n",
    "                    break\n",
    "\n",
    "            if bool_noun:\n",
    "                pairs = get_pairs(sentence)\n",
    "                noun_pairs = []\n",
    "                for pair in pairs:\n",
    "                    if child_token['id'] in pair and child_token['head'] not in pair:\n",
    "                        noun_pairs.append(pair)\n",
    "                        pairs.remove(pair)\n",
    "\n",
    "                qnt_noun_pairs = len(noun_pairs)\n",
    "                for noun_pair in noun_pairs:\n",
    "                    if child_token['head'] > noun_pair[0] and child_token['head'] < noun_pair[1]:\n",
    "                        noun_pairs.remove(noun_pair)\n",
    "                        continue\n",
    "                    for pair in pairs:\n",
    "                        if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                            noun_pairs.remove(noun_pair)\n",
    "\n",
    "                if qnt_noun_pairs > len(noun_pairs):\n",
    "                    bool_new_group = True\n",
    "\n",
    "                    group_token.sort(key=lambda token: token['id'])\n",
    "                    phrase = ''\n",
    "                    for token in group_token:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "\n",
    "                    list_one_step_children_token = []\n",
    "                    for token in group_token:\n",
    "                        list_one_step_children_token.extend(get_list_one_step_children_token(token, sentence))\n",
    "\n",
    "                    for token in list_one_step_children_token:\n",
    "                        if token not in group_token:\n",
    "                            token['head'] = len(sentence) + 1\n",
    "                    sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г_6', root_token['head'], '_'))\n",
    "                    root_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_15_16(sentence: TokenList):\n",
    "    pronoun = ['Я','МЫ','ТЫ','ВЫ','ОН','ОНО','ОНА','ОНИ',\n",
    "            'МЕНЯ','НАС','ТЕБЯ','ВАС','ЕЕ','ЕЙ','ЕЁ','ИХ',\n",
    "            'МНЕ','ТЕБЕ','ВАМ','ЕМУ','ЕЁ','ИМ',\n",
    "            'МЕНЯ','НАС','ВАС','ЕГО',\n",
    "            'НАМИ','ТОБОЙ','ВАМИ','ЕЮ','ИМИ', 'СЕЙ']\n",
    "    introductory_groups = []\n",
    "    for token in sentence:\n",
    "        bool_new_group = False\n",
    "        group = []\n",
    "        if token['deprel'] == 'вводн' and token['form'] != '_':\n",
    "            if sentence[token['head'] - 1]['xpos'] not in ['Г15', 'Г16']:\n",
    "                group = [token]\n",
    "                group.extend(get_list_children_token(token, sentence))\n",
    "                introductory_groups.append(group)\n",
    "\n",
    "    introductory_groups_15 = []\n",
    "    introductory_groups_16 = []\n",
    "    for group in introductory_groups:\n",
    "        bool_15 = True\n",
    "        bool_16 = True\n",
    "        for token in group:\n",
    "            if token['deprel'] in ['предик']:\n",
    "                bool_15 = False\n",
    "                bool_16 = False\n",
    "            if token['deprel'] in ['аппоз', 'об-аппоз']:\n",
    "                bool_15 = False\n",
    "            if not check_feat_token(token, 'ИМ') or check_feat_token(token, 'ЗВ'):\n",
    "                bool_16 = False\n",
    "            if token['upos'] in ['VERB']:\n",
    "                bool_16 = False\n",
    "            if token['lemma'] in pronoun:\n",
    "                bool_15 = False\n",
    "            if token['upos'] in ['ADJ'] and len(group) == 1:\n",
    "                bool_15 = False\n",
    "        if bool_15:\n",
    "            bool_new_group = True\n",
    "            introductory_groups_15.append(group)\n",
    "        elif bool_16:\n",
    "            bool_new_group = True\n",
    "            introductory_groups_16.append(group)\n",
    "\n",
    "    if bool_new_group:\n",
    "        id_new_group = len(sentence) + 1\n",
    "\n",
    "        check_group_15 = []\n",
    "        for group in introductory_groups_15:\n",
    "            for token in group:\n",
    "                check_group_15.append(token)\n",
    "        check_group_16 = []\n",
    "        for group in introductory_groups_16:\n",
    "            for token in group:\n",
    "                check_group_16.append(token)\n",
    "\n",
    "        group = []\n",
    "        if introductory_groups_15 != []:\n",
    "            for token in sentence:\n",
    "                if token['head'] == 1:\n",
    "                    token['head'] = id_new_group\n",
    "                if token not in check_group_15 and token['form'] != '_':\n",
    "                    group.append(token)\n",
    "\n",
    "            phrase = ''\n",
    "            group.sort(key=lambda token: token['id'])\n",
    "            for token in group:\n",
    "                phrase += token['form'] + ' '\n",
    "            phrase = phrase[:-1]\n",
    "            sentence.append(create_ssg(id_new_group, phrase, 'Г15', 1, '_'))\n",
    "            id_15_group = len(sentence)\n",
    "\n",
    "            for introductory_group in introductory_groups_15:\n",
    "                if len(introductory_group) > 1:\n",
    "                    introductory_group.sort(key=lambda token: token['id'])\n",
    "                    introductory_group[0]['deprel'] = '_'\n",
    "                    introductory_group[0]['head'] = len(sentence) + 1\n",
    "                    phrase = \"\"\n",
    "                    for token in introductory_group:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "                    sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г15', id_new_group, 'вводн'))\n",
    "                introductory_group[0]['head'] = len(sentence)\n",
    "\n",
    "        group = []\n",
    "        if introductory_groups_16 != []:\n",
    "            if introductory_groups_15 != []:\n",
    "                for token in sentence:\n",
    "                    if token['head'] == id_new_group and token['deprel'] == '_':\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                    if token not in check_group_16 and token not in check_group_15 and token['form'] != '_':\n",
    "                        group.append(token)\n",
    "                id_new_group = len(sentence) + 1\n",
    "            else:\n",
    "                for token in sentence:\n",
    "                    if token['head'] == 1:\n",
    "                            token['head'] = id_new_group\n",
    "                    if token not in check_group_16 and token not in check_group_15 and token['form'] != '_':\n",
    "                        group.append(token)\n",
    "\n",
    "            phrase = ''\n",
    "            group.sort(key=lambda token: token['id'])\n",
    "            for token in group:\n",
    "                phrase += token['form'] + ' '\n",
    "            phrase = phrase[:-1]\n",
    "            if introductory_groups_15 != []:\n",
    "                sentence.append(create_ssg(id_new_group, phrase, 'Г16', id_15_group, '_'))\n",
    "            else:\n",
    "                sentence.append(create_ssg(id_new_group, phrase, 'Г16', 1, '_'))\n",
    "\n",
    "            for introductory_group in introductory_groups_16:\n",
    "                if len(introductory_group) > 1:\n",
    "                    introductory_group.sort(key=lambda token: token['id'])\n",
    "                    introductory_group[0]['deprel'] = '_'\n",
    "                    introductory_group[0]['head'] = len(sentence) + 1\n",
    "                    phrase = \"\"\n",
    "                    for token in introductory_group:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "                    sentence.append(create_ssg(len(sentence) + 1, phrase, 'Г16', id_new_group, 'вводн'))\n",
    "                introductory_group[0]['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "посещать.\n",
      "111\n",
      "1\n",
      "хочет\n",
      "111\n",
      "0\n",
      "True\n",
      "222\n",
      "посещать.\n",
      "111\n",
      "10\n",
      "хочет\n",
      "111\n",
      "9\n",
      "True\n",
      "222\n",
      "посещать.\n",
      "111\n",
      "10\n",
      "хочет\n",
      "111\n",
      "9\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "data = open(\"input_test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "#for i in range(len(sentences)):\n",
    "group_token = sentences[0]\n",
    "\n",
    "group_token = add_root_token(group_token)\n",
    "\n",
    "bool_while = True\n",
    "while bool_while:\n",
    "    bool_while = False\n",
    "\n",
    "    #bool_new_group = rule_g_15_16(group_token)\n",
    "    if bool_new_group:\n",
    "        bool_while = True\n",
    "\n",
    "    #bool_new_group = rule_g_5(group_token)\n",
    "    if bool_new_group:\n",
    "        bool_while = True\n",
    "\n",
    "    bool_new_group = rule_g_6(group_token)\n",
    "    if bool_new_group:\n",
    "        bool_while = True\n",
    "\n",
    "    bool_new_group = rule_g_3(group_token)\n",
    "    if bool_new_group:\n",
    "        bool_while = True\n",
    "\n",
    "    #bool_new_group = rule_g_4(group_token)\n",
    "    if bool_new_group:\n",
    "        bool_while = True\n",
    "\n",
    "    print(bool_while)\n",
    "\n",
    "with open('output_test.conllu', mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([sentence.serialize() + \"\\n\" for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"input_test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "group_token = sentences[0]\n",
    "group_token[0]\n",
    "get_phrase_tokens(group_token[0], group_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False\n",
    "if a * b:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_head_token(token: Token, sentence: TokenList) -> Token:\n",
    "    head_token = sentence[token['head'] - 1]\n",
    "    if head_token['form'] != '_' or head_token['id'] == 1:\n",
    "        return head_token['id']\n",
    "    else:\n",
    "        for token in sentence:\n",
    "            if token['head'] == head_token['id'] and token['deprel'] == '_':\n",
    "                return token['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conllu.models.TokenList"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"input_test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "#for i in range(len(sentences)):\n",
    "sentence = sentences[0]\n",
    "\n",
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
