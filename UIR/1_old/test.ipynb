{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово 0\n"
     ]
    }
   ],
   "source": [
    "from proc.func_file import *\n",
    "from conllu import parse\n",
    "from conllu.models import TokenList\n",
    "\n",
    "name_file = \"test\"\n",
    "data = read_file(f\"output/{name_file}.conllu\")\n",
    "sentences = parse(data.read())\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    #print(sentence.metadata['sent_id'])\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=\"ssg_conllu\",\n",
    "            user=\"postgres\",\n",
    "            password=\"postgres\"\n",
    "        )\n",
    "\n",
    "        cur = conn.cursor()\n",
    "        str_insert = f\"INSERT INTO sentence (sent_id) VALUES (%s)\"\n",
    "        cur.execute(str_insert, (sentence.metadata['sent_id'],))\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        for token in sentence:\n",
    "            if token['form'] == None or token['form'] == '_':\n",
    "                token_form = None\n",
    "            else:\n",
    "                token_form = token['form']\n",
    "\n",
    "            if token['lemma'] == None or token['lemma'] == '_':\n",
    "                token_lemma = None\n",
    "            else:\n",
    "                token_lemma = token['lemma']\n",
    "\n",
    "            if token['xpos'] == None or token['xpos'] == '_':\n",
    "                token_xpos = None\n",
    "            else:\n",
    "                token_xpos = token['xpos']\n",
    "\n",
    "            if token['feats'] == None or token['feats'] == '_':\n",
    "                token_feats = None\n",
    "            else:\n",
    "                token_feats = list(token['feats'].keys())[0]\n",
    "\n",
    "            if token['deprel'] == None or token['deprel'] == '_':\n",
    "                token_deprel = None\n",
    "            else:\n",
    "                token_deprel = token['deprel']\n",
    "\n",
    "            if token['deps'] == None or token['deps'] == '_':\n",
    "                token_deps = None\n",
    "            else:\n",
    "                token_deps = token['deps']\n",
    "\n",
    "            if token['misc'] == None or token['misc'] == '_':\n",
    "                token_misc = None\n",
    "            else:\n",
    "                token_misc = token['misc']\n",
    "\n",
    "            str_insert = f\"INSERT INTO token (id_sentence, id, form, lemma, upos, xpos, feats, head, deprel, deps, misc) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            cur.execute(str_insert, (i+1, token['id'], token_form, token_lemma, token['upos'], token_xpos, token_feats, token['head'], token_deprel, token_deps, token_misc))\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        #db_res = cur.fetchone()\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Готово {i}\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Одной по улице бабушка строго запретила ей ходить.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"ssg_conllu\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\"\n",
    "    )\n",
    "\n",
    "    #form = input(\"Введите слово или словосочетание: \")\n",
    "    form = \"ОдНой по\"\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(f\"SELECT upos, id_sentence FROM token WHERE LOWER(upos) LIKE LOWER('%{form}%')\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    db_res = cur.fetchall()\n",
    "\n",
    "    if db_res == []:\n",
    "        print('Ничего не найдено')\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    else:\n",
    "        with open(\"output/result_search.conllu\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "            cur = conn.cursor()\n",
    "            for row in db_res:\n",
    "                print(row[0])\n",
    "\n",
    "                cur.execute(f\"SELECT sent_id FROM sentence WHERE id_sentence = {row[1]}\")\n",
    "                conn.commit()\n",
    "                db_res_w = cur.fetchall()\n",
    "                file.writelines(f\"# sent_id = {db_res_w[0][0]}\\n\")\n",
    "\n",
    "                cur.execute(f\"SELECT id, form, lemma, upos, xpos, feats, head, deprel, deps, misc FROM token WHERE id_sentence = {row[1]} ORDER BY id\")\n",
    "                conn.commit()\n",
    "                db_res_w = cur.fetchall()\n",
    "                for row_w in db_res_w:\n",
    "                    str = \"\"\n",
    "                    for attr in row_w:\n",
    "                        if attr == None:\n",
    "                            attr = '_'\n",
    "                        str += f\"{attr}\t\"\n",
    "                    str = str[:-1]\n",
    "                    file.writelines(f'{str}\\n')\n",
    "                file.writelines('\\n')\n",
    "\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(s: str):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Лемма\n",
      "2. Словоформа\n",
      "3. Критерий\n",
      "4. Морфологические признаки\n",
      "5. Синтаксическое отношение\n",
      "6. Перейти к следующему слову\n"
     ]
    }
   ],
   "source": [
    "qnt = input(\"Введите количество слов или словосочетаний, по которым будет вестись поиск: \")\n",
    "if is_int(qnt):\n",
    "    int_qnt = int(qnt)\n",
    "    if int_qnt > 0:\n",
    "        #os.system('cls')\n",
    "        for i in range(int_qnt):\n",
    "            lemma = None\n",
    "            form = None\n",
    "            feats = None\n",
    "            xpos = None\n",
    "            head = None\n",
    "            deprel = None\n",
    "            menu = 0\n",
    "            print(\"1. Слово\")\n",
    "            print(\"2. Слосочетание\")\n",
    "            menu = input(f\"Введите номер пункта для {i+1}-ого слова/словосочетания: \")\n",
    "            match menu:\n",
    "                case \"1\":\n",
    "                    while menu != 6:\n",
    "                        print(\"1. Лемма\")\n",
    "                        print(\"2. Словоформа\")\n",
    "                        print(\"3. Морфологические признаки\")\n",
    "                        print(\"4. Синтаксическое отношение\")\n",
    "                        print(\"5. Посмотреть текущие ограничения\")\n",
    "                        print(\"6. Перейти к следующему слову/словосочетанию\")\n",
    "                        menu = input(f\"Введите номер пункта для {i+1}-ого слова/словосочетания: \")\n",
    "\n",
    "                        match menu:\n",
    "                            case \"1\":\n",
    "                                lemma = input(f\"Введите лемму: \")\n",
    "\n",
    "                            case \"2\":\n",
    "                                form = input(f\"Введите словоформу: \")\n",
    "\n",
    "                            case \"2\":\n",
    "                                form = input(f\"Введите словоформу: \")\n",
    "\n",
    "\n",
    "                case \"2\":\n",
    "                    print(\"1. Точная форма\")\n",
    "                    print(\"2. Критерий\")\n",
    "                    print(\"3. Синтаксическое отношение\")\n",
    "                    print(\"4. Посмотреть текущие ограничения\")\n",
    "                    print(\"5. Перейти к следующему слову\")\n",
    "\n",
    "                case _:\n",
    "                    print(\"Ошибка: данного пункта нет\")\n",
    "            while menu == 0:\n",
    "                menu = input(f\"Введите номер пункта для {i+1}-ого слова/словосочетания: \")\n",
    "                pass\n",
    "else:\n",
    "    print('Ошибка: введено не натуральное число')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Это', 'пример', 'предложения.']\n"
     ]
    }
   ],
   "source": [
    "строка = \"Это пример предложения.\"\n",
    "слова = строка.split()\n",
    "print(слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WITH t1 AS(SELECT * FROM token WHERE LOWER(lemma) = LOWER('бабушка') AND LOWER(feats) = LOWER('%S%') AND LOWER(feats) LIKE LOWER('%ОД%') AND LOWER(feats) NOT LIKE LOWER('%НЕОД%') AND LOWER(deprel) = LOWER('пред')\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"WITH t1 AS(SELECT * FROM token WHERE LOWER(lemma) = LOWER('бабушка') AND LOWER(feats) = LOWER('%S%') AND LOWER(feats) LIKE LOWER('%ОД%') AND LOWER(feats) NOT LIKE LOWER('%НЕОД%') AND LOWER(deprel) = LOWER('пред') AND \"\n",
    "a[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = False + True\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu.models import TokenList\n",
    "from conllu import parse\n",
    "from proc.func_conllu import *\n",
    "\n",
    "def rule_g_5(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "        #Поиск ИНФ, который не завист от другого ИНФ\n",
    "        if check_feat(token, 'ИНФ') and not check_feat(sentence[token['head'] - 1], 'ИНФ'):\n",
    "            head_token = token\n",
    "            last_inf_token = token\n",
    "            while head_token['deprel'] == '_' and head_token['head'] != 1:\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "            group_token = [head_token]\n",
    "            inf_group_token = [head_token]\n",
    "            bool_N = False\n",
    "            while True:\n",
    "                if head_token['head'] == 1:\n",
    "                    break\n",
    "\n",
    "                buf_head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "                if buf_head_token['deprel'] in ['сочин', 'сент-соч', 'соч-союзн', 'кратн']:\n",
    "                    break\n",
    "\n",
    "                if buf_head_token['form'] == '_':\n",
    "                    buf_head_token = get_head_component_sg(buf_head_token, sentence)\n",
    "\n",
    "                if buf_head_token['id'] > head_token['id']:\n",
    "                    break\n",
    "\n",
    "                if buf_head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "                    if buf_head_token['upos'] in ['NOUN', 'ADJ', 'ADV']:\n",
    "                        if bool_N:\n",
    "                            break\n",
    "                        bool_N = True\n",
    "                    head_token = sentence[head_token['head'] - 1]\n",
    "                    group_token.append(head_token)\n",
    "                    if check_feat(buf_head_token, 'ИНФ'):\n",
    "                        inf_group_token.append(buf_head_token)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if head_token['xpos'] != 'Г5' or sentence[token['head'] - 1]['xpos'] != 'Г5' and head_token['deprel'] == '_':\n",
    "                #Поиск всех зависимых ИНФ от найденного ИНФ\n",
    "                bool_inf_child = True\n",
    "                while bool_inf_child:\n",
    "                    bool_inf_child = False\n",
    "                    for token in sentence:\n",
    "                        if token['head'] == last_inf_token['id']:\n",
    "                            if check_feat(token, 'ИНФ'):\n",
    "                                group_token.append(token)\n",
    "                                inf_group_token.append(token)\n",
    "                                last_inf_token = token\n",
    "                                bool_inf_child = True\n",
    "                                break\n",
    "\n",
    "            bool_check_g5 = True\n",
    "            if len(group_token) == 2:\n",
    "                for token in group_token:\n",
    "                    if not check_feat(token, 'ИНФ'):\n",
    "                        bool_check_g5 = False\n",
    "            if len(group_token) < 2:\n",
    "                bool_check_g5 = False\n",
    "\n",
    "            if bool_check_g5:\n",
    "                #Поиск двух наборов пар связей (в первой все связи, в которых отсуствует найденный ИНФ, а во второй - присуствует)\n",
    "                pairs = get_sorted_pairs(sentence)\n",
    "                buf_pairs = pairs.copy()\n",
    "                inf_pairs = []\n",
    "                for pair in buf_pairs:\n",
    "                    if last_inf_token['id'] in pair and last_inf_token['head'] not in pair:\n",
    "                        inf_pairs.append(pair)\n",
    "                        pairs.remove(pair)\n",
    "\n",
    "                #Удалить все пары с ИНФ, которые нарушают проективность\n",
    "                qnt_inf_pairs = len(inf_pairs)\n",
    "                buf_inf_pairs = inf_pairs.copy()\n",
    "                for inf_pair in buf_inf_pairs:\n",
    "                    if last_inf_token['head'] > inf_pair[0] and last_inf_token['head'] < inf_pair[1]:\n",
    "                        inf_pairs.remove(inf_pair)\n",
    "                        continue\n",
    "                    for pair in pairs:\n",
    "                        if (inf_pair[0] > pair[0] and inf_pair[0] < pair[1] and (inf_pair[1] < pair[0] or inf_pair[1] > pair[1])) or (inf_pair[1] > pair[0] and inf_pair[1] < pair[1] and (inf_pair[0] < pair[0] or inf_pair[0] > pair[1])):\n",
    "                            inf_pairs.remove(inf_pair)\n",
    "\n",
    "                #Проверка, что нужно добавлять новую группу\n",
    "                if qnt_inf_pairs > len(inf_pairs):\n",
    "                    bool_new_group = True\n",
    "\n",
    "                    #Дополнить набор слов, которые образуют новую группу\n",
    "                    for pair in inf_pairs:\n",
    "                        if pair[0] != last_inf_token['id']:\n",
    "                            token = sentence[pair[0] - 1]\n",
    "                        else:\n",
    "                            token = sentence[pair[1] - 1]\n",
    "                        group_token.append(token)\n",
    "                        group_token.extend(get_group_child_token(token, sentence))\n",
    "\n",
    "                    #Получить текст группы\n",
    "                    buf_group_token = group_token.copy()\n",
    "                    for token in buf_group_token:\n",
    "                        if token['form'] == '_':\n",
    "                            group_token.remove(token)\n",
    "                            group_token.extend(get_components_sg(token, sentence))\n",
    "                    group_token.sort(key=lambda token: token['id'])\n",
    "                    phrase = ''\n",
    "                    for token in group_token:\n",
    "                        phrase += token['form'] + ' '\n",
    "                    phrase = phrase[:-1]\n",
    "\n",
    "                    group_token = buf_group_token.copy()\n",
    "\n",
    "                    #Получить все слова, которые связаны со словами из группы\n",
    "                    list_one_step_children_token = []\n",
    "                    for token in group_token:\n",
    "                        list_one_step_children_token.extend(get_group_one_step_child_token(token, sentence))\n",
    "\n",
    "                    #Добавить новую группу и изменить связи между элементами ССГ\n",
    "                    for token in list_one_step_children_token:\n",
    "                        if token not in group_token:\n",
    "                            token['head'] = len(sentence) + 1\n",
    "                    sentence.append(create_sg(len(sentence) + 1, phrase, 'Г5', head_token['head'], head_token['deprel']))\n",
    "                    head_token['deprel'] = '_'\n",
    "                    head_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_g_6(sentence: TokenList):\n",
    "    bool_new_group = False\n",
    "    for token in sentence:\n",
    "        #поиск ГЛ, который не является ИНФ\n",
    "        if token['upos'] == 'VERB' and not check_feat(token, 'ИНФ'):\n",
    "            head_token = token\n",
    "            while head_token['deprel'] == '_' and head_token['head'] != 1:\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "            if head_token['xpos'] != 'Г6' or sentence[token['head'] - 1]['xpos'] != 'Г6' and head_token['deprel'] == '_':\n",
    "                #поиск ИНФ, который зависит от найденного слова\n",
    "                for token in get_group_one_step_child_token(head_token, sentence):\n",
    "                    buf_token = token\n",
    "                    if buf_token['form'] == '_':\n",
    "                        buf_token = get_head_component_sg(buf_token, sentence)\n",
    "                    if buf_token['upos'] in ['PRON', 'NOUN', 'ADJ', 'ADV']:\n",
    "                        noun_token = token\n",
    "\n",
    "                        #Поиск двух наборов пар связей (в первой все связи, в которых отсуствует найденный СУЩ, а во второй - присуствует)\n",
    "                        pairs = get_sorted_pairs(sentence)\n",
    "                        noun_pairs = []\n",
    "                        buf_pairs = pairs.copy()\n",
    "                        if noun_token['form'] == '_':\n",
    "                            id_noun_token = get_head_component_sg(noun_token, sentence)['id']\n",
    "                            for token in sentence:\n",
    "                                if token['head'] == id_noun_token:\n",
    "                                    id_child_token = token['id']\n",
    "                                    break\n",
    "                            for pair in buf_pairs:\n",
    "                                if id_noun_token in pair and get_id_head_token(noun_token, sentence) not in pair and id_child_token not in pair:\n",
    "                                    noun_pairs.append(pair)\n",
    "                                    pairs.remove(pair)\n",
    "                        else:\n",
    "                            id_noun_token = noun_token['id']\n",
    "                            for pair in buf_pairs:\n",
    "                                if id_noun_token in pair and get_id_head_token(noun_token, sentence) not in pair:\n",
    "                                    noun_pairs.append(pair)\n",
    "                                    pairs.remove(pair)\n",
    "\n",
    "                        #Проверить, что существуют пары с ИНФ\n",
    "                        qnt_noun_pairs = len(noun_pairs)\n",
    "                        if qnt_noun_pairs > 0:\n",
    "\n",
    "                            #Удалить все пары с ИНФ, которые нарушают проективность\n",
    "                            buf_noun_pairs = noun_pairs.copy()\n",
    "                            for noun_pair in buf_noun_pairs:\n",
    "                                if get_id_head_token(noun_token, sentence) > noun_pair[0] and get_id_head_token(noun_token, sentence) < noun_pair[1]:\n",
    "                                    noun_pairs.remove(noun_pair)\n",
    "                                    continue\n",
    "                                for pair in pairs:\n",
    "                                    if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                                        noun_pairs.remove(noun_pair)\n",
    "\n",
    "                            #Проверка, что нужно добавлять новую группу\n",
    "                            if qnt_noun_pairs > len(noun_pairs):\n",
    "                                bool_new_group = True\n",
    "\n",
    "                                #Получить набор слов, которые образуют новую группу\n",
    "                                group_token = [head_token, noun_token]\n",
    "                                for pair in noun_pairs:\n",
    "                                    if pair[0] != noun_token['id']:\n",
    "                                        token = sentence[pair[0] - 1]\n",
    "                                    else:\n",
    "                                        token = sentence[pair[1] - 1]\n",
    "                                    group_token.append(token)\n",
    "                                    group_token.extend(get_group_child_token(token, sentence))\n",
    "\n",
    "                                #Получить словосочетание\n",
    "                                buf_group_token = group_token.copy()\n",
    "                                for token in buf_group_token:\n",
    "                                    if token['form'] == '_':\n",
    "                                        group_token.remove(token)\n",
    "                                        group_token.extend(get_components_sg(token, sentence))\n",
    "                                group_token.sort(key=lambda token: token['id'])\n",
    "                                phrase = ''\n",
    "                                for token in group_token:\n",
    "                                    phrase += token['form'] + ' '\n",
    "                                phrase = phrase[:-1]\n",
    "                                group_token = buf_group_token.copy()\n",
    "\n",
    "                                #Получить все слова, которые связаны со словами из группы\n",
    "                                group_one_step_child_token = []\n",
    "                                for token in group_token:\n",
    "                                    group_one_step_child_token.extend(get_group_one_step_child_token(token, sentence))\n",
    "\n",
    "                                #Добавить новую группу и изменить связи между элементами ССГ\n",
    "                                for token in group_one_step_child_token:\n",
    "                                    if token not in group_token:\n",
    "                                        token['head'] = len(sentence) + 1\n",
    "                                sentence.append(create_sg(len(sentence) + 1, phrase, 'Г6', head_token['head'], head_token['deprel']))\n",
    "                                head_token['deprel'] = '_'\n",
    "                                head_token['head'] = len(sentence)\n",
    "\n",
    "        #Поиск ИНФ, который не завист от другого ИНФ\n",
    "        if check_feat(token, 'ИНФ') and not check_feat(sentence[token['head'] - 1], 'ИНФ'):\n",
    "            head_token = token\n",
    "            last_inf_token = token\n",
    "            while head_token['deprel'] == '_' and head_token['head'] != 1:\n",
    "                head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "            group_token = [head_token]\n",
    "            inf_group_token = [head_token]\n",
    "            bool_N = False\n",
    "            while True:\n",
    "                if head_token['head'] == 1:\n",
    "                    break\n",
    "\n",
    "                buf_head_token = sentence[head_token['head'] - 1]\n",
    "\n",
    "                if buf_head_token['deprel'] in ['сочин', 'сент-соч', 'соч-союзн', 'кратн']:\n",
    "                    break\n",
    "\n",
    "                if buf_head_token['form'] == '_':\n",
    "                    buf_head_token = get_head_component_sg(buf_head_token, sentence)\n",
    "\n",
    "                if buf_head_token['id'] > head_token['id']:\n",
    "                    break\n",
    "\n",
    "                if buf_head_token['upos'] in ['VERB', 'NOUN', 'ADJ', 'ADV']:\n",
    "                    if buf_head_token['upos'] in ['NOUN', 'ADJ', 'ADV']:\n",
    "                        if bool_N:\n",
    "                            break\n",
    "                        bool_N = True\n",
    "                    head_token = sentence[head_token['head'] - 1]\n",
    "                    group_token.append(head_token)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if head_token['xpos'] != 'Г6' or sentence[token['head'] - 1]['xpos'] != 'Г6' and head_token['deprel'] == '_':\n",
    "                #Поиск всех зависимых ИНФ от найденного ИНФ\n",
    "                bool_inf_child = True\n",
    "                while bool_inf_child:\n",
    "                    bool_inf_child = False\n",
    "                    for token in sentence:\n",
    "                        if token['head'] == last_inf_token['id']:\n",
    "                            if check_feat(token, 'ИНФ'):\n",
    "                                group_token.append(token)\n",
    "                                inf_group_token.append(token)\n",
    "                                last_inf_token = token\n",
    "                                bool_inf_child = True\n",
    "                                break\n",
    "\n",
    "                for noun_token in get_group_child_token(last_inf_token, sentence):\n",
    "                    bool_noun = False\n",
    "                    if noun_token['form'] == '_':\n",
    "                        if get_head_component_sg(noun_token, sentence)['upos'] in ['NOUN', 'ADJ', 'ADV']:\n",
    "                            bool_noun = True\n",
    "                    else:\n",
    "                        if noun_token['upos'] in ['NOUN', 'ADJ', 'ADV']:\n",
    "                            bool_noun = True\n",
    "\n",
    "                    if bool_noun:\n",
    "                        group_token.append(noun_token)\n",
    "                        #Поиск двух наборов пар связей (в первой все связи, в которых отсуствует найденный СУЩ, а во второй - присуствует)\n",
    "                        pairs = get_sorted_pairs(sentence)\n",
    "                        buf_pairs = pairs.copy()\n",
    "                        noun_pairs = []\n",
    "                        for pair in buf_pairs:\n",
    "                            if noun_token['id'] in pair and noun_token['head'] not in pair:\n",
    "                                noun_pairs.append(pair)\n",
    "                                pairs.remove(pair)\n",
    "\n",
    "                        #Удалить все пары с СУЩ, которые нарушают проективность\n",
    "                        qnt_noun_pairs = len(noun_pairs)\n",
    "                        buf_noun_pairs = noun_pairs.copy()\n",
    "                        for noun_pair in buf_noun_pairs:\n",
    "                            if noun_token['head'] > noun_pair[0] and noun_token['head'] < noun_pair[1]:\n",
    "                                noun_pairs.remove(noun_pair)\n",
    "                                continue\n",
    "                            for pair in pairs:\n",
    "                                if (noun_pair[0] > pair[0] and noun_pair[0] < pair[1] and (noun_pair[1] < pair[0] or noun_pair[1] > pair[1])) or (noun_pair[1] > pair[0] and noun_pair[1] < pair[1] and (noun_pair[0] < pair[0] or noun_pair[0] > pair[1])):\n",
    "                                    noun_pairs.remove(noun_pair)\n",
    "\n",
    "                        #Проверка, что нужно добавлять новую группу\n",
    "                        if qnt_noun_pairs > len(noun_pairs):\n",
    "                            bool_new_group = True\n",
    "\n",
    "                            #Дополнить набор слов, которые образуют новую группу\n",
    "                            for pair in noun_pairs:\n",
    "                                if pair[0] != noun_token['id']:\n",
    "                                    token = sentence[pair[0] - 1]\n",
    "                                else:\n",
    "                                    token = sentence[pair[1] - 1]\n",
    "                                group_token.append(token)\n",
    "                                group_token.extend(get_group_child_token(token, sentence))\n",
    "\n",
    "                            #Получить текст группы\n",
    "                            buf_group_token = group_token.copy()\n",
    "                            for token in buf_group_token:\n",
    "                                if token['form'] == '_':\n",
    "                                    group_token.remove(token)\n",
    "                                    group_token.extend(get_components_sg(token, sentence))\n",
    "                            group_token.sort(key=lambda token: token['id'])\n",
    "                            phrase = ''\n",
    "                            for token in group_token:\n",
    "                                phrase += token['form'] + ' '\n",
    "                            phrase = phrase[:-1]\n",
    "                            group_token = buf_group_token.copy()\n",
    "\n",
    "                            #Получить все слова, которые связаны со словами из группы\n",
    "                            list_one_step_children_token = []\n",
    "                            for token in group_token:\n",
    "                                list_one_step_children_token.extend(get_group_one_step_child_token(token, sentence))\n",
    "\n",
    "                            #Добавить новую группу и изменить связи между элементами ССГ\n",
    "                            for token in list_one_step_children_token:\n",
    "                                if token not in group_token:\n",
    "                                    token['head'] = len(sentence) + 1\n",
    "                            sentence.append(create_sg(len(sentence) + 1, phrase, 'Г6', head_token['head'], head_token['deprel']))\n",
    "                            head_token['deprel'] = '_'\n",
    "                            head_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"input/test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "bool_new_group = False\n",
    "for sentence in sentences:\n",
    "    add_root_token(sentence)\n",
    "\n",
    "    bool_new_group += rule_g_6(sentence)\n",
    "\n",
    "with open(\"output/test.conllu\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([sentence.serialize() + \"\\n\" for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sentences \u001b[39m=\u001b[39m parse(data\u001b[39m.\u001b[39mread())\n\u001b[0;32m      5\u001b[0m bool_new_group \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m sentence \u001b[39m=\u001b[39m sentences[\u001b[39m6\u001b[39;49m]\n\u001b[0;32m      7\u001b[0m get_group_child_token(sentence[\u001b[39m18\u001b[39m], sentence)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\conllu\\models.py:342\u001b[0m, in \u001b[0;36mSentenceList.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m SentenceList(\n\u001b[0;32m    338\u001b[0m         sentences\u001b[39m=\u001b[39m\u001b[39msuper\u001b[39m(SentenceList, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(key),\n\u001b[0;32m    339\u001b[0m         metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\n\u001b[0;32m    340\u001b[0m     )\n\u001b[0;32m    341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SentenceList, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data = open(\"input/test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "bool_new_group = False\n",
    "sentence = sentences[6]\n",
    "get_group_child_token(sentence[18], sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_group_child_token(sentence[18], sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 19,\n",
       " 'form': 'в',\n",
       " 'lemma': 'В',\n",
       " 'upos': 'ADP',\n",
       " 'xpos': 'PR',\n",
       " 'feats': {'PR': ''},\n",
       " 'head': 18,\n",
       " 'deprel': '2-компл',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "if [2, 3, 4, 5] not in [3, 7]:\n",
    "    print('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "талант\n",
      "обыск\n",
      "спать\n",
      "ранить\n",
      "власть\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"wordbook.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    wb = json.load(json_file)\n",
    "\n",
    "for i in range(len(wb['words'])):\n",
    "    print(wb['words'][i]['lemma'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from conllu.models import TokenList\n",
    "from conllu import parse\n",
    "from proc.func_conllu import *\n",
    "\n",
    "def rule_g_1(sentence: TokenList):\n",
    "    for head_token in sentence:\n",
    "        bool_new_group = False\n",
    "        magn_token = None\n",
    "        if (head_token['xpos'] != 'Г1' or sentence[head_token['head'] - 1]['xpos'] != 'Г1' and head_token['deprel'] == '_') and head_token['id'] != 1:\n",
    "            if head_token['form'] == '_':\n",
    "                group_component_sg = get_components_sg(head_token, sentence)\n",
    "                if len(group_component_sg) == 2:\n",
    "                    for token in group_component_sg:\n",
    "                        if token['head'] == head_token['id'] and token['deprel'] == '_':\n",
    "                            lemma = token['lemma']\n",
    "                else:\n",
    "                    lemma = get_head_component_sg(head_token, sentence)['lemma']\n",
    "            else:\n",
    "                lemma = head_token['lemma']\n",
    "            group_child_token = get_group_child_token(head_token, sentence)\n",
    "            if len(group_child_token) > 0:\n",
    "                with open(\"wordbook.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "                    wordbook = json.load(json_file)\n",
    "                    for word in wordbook['words']:\n",
    "                        if word['lemma'] == lemma.lower():\n",
    "                            for child_token in group_child_token:\n",
    "                                for magn in word['magn']:\n",
    "                                    if magn == child_token['lemma'].lower():\n",
    "                                        bool_new_group = True\n",
    "                                        #print('1', bool_new_group)\n",
    "                                        magn_token = child_token.copy()\n",
    "                                        break\n",
    "            if bool_new_group:\n",
    "                group_token = [head_token]\n",
    "                for token in get_group_child_token(head_token, sentence):\n",
    "                    if token['deprel'] not in ['сравнит', 'сочин', 'сент-соч', 'соч-союзн', 'кратн']:\n",
    "                        group_token.append(token)\n",
    "                buf_group_token = group_token.copy()\n",
    "                for token in buf_group_token:\n",
    "                    if token['form'] == '_':\n",
    "                        group_token.remove(token)\n",
    "                        group_token.extend(get_components_sg(token, sentence))\n",
    "                group_token.sort(key=lambda token: token['id'])\n",
    "                phrase = ''\n",
    "                for token in group_token:\n",
    "                    phrase += token['form'] + ' '\n",
    "                phrase = phrase[:-1]\n",
    "                group_token = buf_group_token.copy()\n",
    "\n",
    "                #Добавить новую группу и изменить связи между элементами ССГ\n",
    "                for token in group_token:\n",
    "                    if token not in group_token:\n",
    "                        token['head'] = len(sentence) + 1\n",
    "                sentence.append(create_sg(len(sentence) + 1, phrase, 'Г1', head_token['head'], head_token['deprel']))\n",
    "                head_token['deprel'] = '_'\n",
    "                head_token['head'] = len(sentence)\n",
    "\n",
    "    return bool_new_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"conllu/tree/test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "bool_new_group = False\n",
    "for sentence in sentences:\n",
    "    add_root_token(sentence)\n",
    "\n",
    "    bool_new_group += rule_g_1(sentence)\n",
    "\n",
    "with open(\"conllu/ssg/test.conllu\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines([sentence.serialize() + \"\\n\" for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'form': 'Сделано',\n",
       " 'lemma': 'СДЕЛАТЬ',\n",
       " 'upos': 'VERB',\n",
       " 'xpos': None,\n",
       " 'feats': None,\n",
       " 'head': 0,\n",
       " 'deprel': '_',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"conllu/tree/test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "bool_new_group = False\n",
    "sentence = sentences[0]\n",
    "sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_group_one_step_child_token(head_token: Token, sentence: TokenList) -> list:\n",
    "    group_one_step_child_token = []\n",
    "    for token in sentence:\n",
    "        if token['head'] == head_token['id'] and token['deprel'] != '_':\n",
    "            group_one_step_child_token.append(token)\n",
    "    return group_one_step_child_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_for_group(token: Token, sentence: TokenList):\n",
    "    if token['deprel'] not in ['сравнит', 'сочин', 'сент-соч', 'соч-союзн', 'кратн']:\n",
    "        group_child_token = [token]\n",
    "        for child_token in _get_group_one_step_child_token(token, sentence):\n",
    "            group_child_token.extend(get_token_for_group(child_token, sentence))\n",
    "        return group_child_token\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'опред'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(\"conllu/tree/test.conllu\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "sentences = parse(data.read())\n",
    "\n",
    "bool_new_group = False\n",
    "sentence = sentences[0]\n",
    "\n",
    "sentence[2]['deps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    }
   ],
   "source": [
    "if [] == None:\n",
    "    print(123)\n",
    "else:\n",
    "    print(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
